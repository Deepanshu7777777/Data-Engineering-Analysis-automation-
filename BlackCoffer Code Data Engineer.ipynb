{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43459b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Deepa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed and saved to Output.xlsx\n"
     ]
    }
   ],
   "source": [
    "#Data Engineer @DRDO\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Download the punkt tokenizer from nltk (if not already)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 1. Data Extraction\n",
    "def extract_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        paragraphs = soup.find_all('p')\n",
    "        return ' '.join([p.get_text() for p in paragraphs])\n",
    "    except:\n",
    "        return \"\"  # Return empty string if there's an issue with the URL\n",
    "\n",
    "# 2. Text Analysis\n",
    "def count_syllables(word):\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower().strip(\".:;?!\")\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def compute_metrics(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Filter out punctuation and set to lowercase\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "    words_lower = [word.lower() for word in words]\n",
    "\n",
    "    # Positive and Negative counts\n",
    "    positive_count = sum(1 for word in words_lower if word in positive_words)\n",
    "    negative_count = sum(1 for word in words_lower if word in negative_words)\n",
    "\n",
    "    # Polarity and Subjectivity\n",
    "    polarity_score = (positive_count - negative_count) / ((positive_count + negative_count) or 1)\n",
    "    subjectivity_score = (positive_count + negative_count) / len(words)\n",
    "\n",
    "    # Average sentence length\n",
    "    avg_sentence_length = len(words) / len(sentences)\n",
    "\n",
    "    # Complex words and their percentages\n",
    "    complex_words = [word for word in words if count_syllables(word) > 2]\n",
    "    percentage_complex = len(complex_words) / len(words)\n",
    "\n",
    "    # Fog index\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex * 100)\n",
    "\n",
    "    # Personal pronouns\n",
    "    personal_pronouns_count = sum(1 for word in words if word in ['I', 'we', 'my', 'ours', 'us'])\n",
    "\n",
    "    return [\n",
    "        positive_count, negative_count, polarity_score, subjectivity_score,\n",
    "        avg_sentence_length, percentage_complex * 100, fog_index,\n",
    "        len(complex_words), len(words), sum(count_syllables(word) for word in words) / len(words),\n",
    "        personal_pronouns_count, sum(len(word) for word in words) / len(words)\n",
    "    ]\n",
    "\n",
    "\n",
    "# Main Execution\n",
    "df_input = pd.read_excel('C:/Users/Deepa/OneDrive/Desktop/.ipynb_checkpoints/Input.xlsx')\n",
    "positive_words = set(open(\"C:/Users/Deepa/OneDrive/Desktop/.ipynb_checkpoints/MasterDictionary/positive-words.txt\").read().splitlines())\n",
    "negative_words = set(open(\"C:/Users/Deepa/OneDrive/Desktop/.ipynb_checkpoints/MasterDictionary/negative-words.txt\").read().splitlines())\n",
    "\n",
    "results = []\n",
    "\n",
    "for _, row in df_input.iterrows():\n",
    "    content = extract_content(row['URL'])\n",
    "    metrics = compute_metrics(content)\n",
    "    results.append([row['URL_ID'], row['URL'], content] + metrics)  # Added the 'content' to results\n",
    "\n",
    "df_output = pd.DataFrame(results, columns=[\n",
    "    \"URL_ID\", \"URL\", \"Content\", \"POSITIVE COUNT\", \"NEGATIVE COUNT\", \"POLARITY SCORE\",\n",
    "    \"SUBJECTIVITY SCORE\", \"AVG SENTENCE LENGTH\", \"PERCENTAGE OF COMPLEX WORDS\", \"FOG INDEX\",\n",
    "    \"COMPLEX WORD COUNT\", \"WORD COUNT\", \"AVG SYLLABLES PER WORD\", \"PERSONAL PRONOUNS COUNT\", \"AVG WORD LENGTH\"\n",
    "])\n",
    "\n",
    "df_output.to_excel('Output.xlsx', index=False)\n",
    "print(\"Analysis completed and saved to Output.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c50d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the above code takes 30 seconds to get all the data analysis and extration of data , \n",
    "##which is very fast in comaprision to toher peopel codes ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6439adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
